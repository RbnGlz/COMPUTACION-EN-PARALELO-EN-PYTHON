{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ca27-56af-457a-8aed-719d88db9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Generación de datos sintéticos\n",
    "# ---------------------------------------------\n",
    "def generar_datos_sinteticos(num_muestras=5000):\n",
    "    \"\"\"\n",
    "    Genera un conjunto de datos sintético.\n",
    "    Cada muestra tiene 10 características y la etiqueta es binaria,\n",
    "    definida de forma simple en función de la suma de las características.\n",
    "    \"\"\"\n",
    "    X = np.random.randn(num_muestras, 10)  # Genera datos aleatorios con distribución normal\n",
    "    y = (np.sum(X, axis=1) > 0).astype(int)  # Etiqueta: 1 si la suma es positiva, 0 en otro caso\n",
    "    return X, y\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Definición de la red neuronal simple\n",
    "# ---------------------------------------------\n",
    "class RedNeuronalSimple(nn.Module):\n",
    "    def __init__(self, dim_entrada=10, dim_oculta=32, dim_salida=2):\n",
    "        super(RedNeuronalSimple, self).__init__()\n",
    "        # Definición de la arquitectura de la red neuronal\n",
    "        self.modelo = nn.Sequential(\n",
    "            nn.Linear(dim_entrada, dim_oculta),  # Capa de entrada a capa oculta\n",
    "            nn.ReLU(),  # Función de activación ReLU\n",
    "            nn.Linear(dim_oculta, dim_salida)  # Capa oculta a capa de salida\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.modelo(x)  # Propagación hacia adelante\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Función de entrenamiento con DataLoader optimizado\n",
    "# ---------------------------------------------\n",
    "def entrenar_modelo(modelo, X_train, y_train, epochs=5, batch_size=128):\n",
    "    \"\"\"\n",
    "    Entrena el modelo usando Adam y CrossEntropyLoss.\n",
    "    Se utiliza DataLoader con 'num_workers' > 0 para paralelizar la carga de datos.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=0.001)  # Optimizador Adam\n",
    "    criterio = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
    "    modelo.train()  # Establecer el modelo en modo de entrenamiento\n",
    "    \n",
    "    # Convertir a tensores y crear un dataset\n",
    "    tensor_X = torch.tensor(X_train, dtype=torch.float32)  # Convertir características a tensor\n",
    "    tensor_y = torch.tensor(y_train, dtype=torch.long)  # Convertir etiquetas a tensor\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_X, tensor_y)  # Crear dataset\n",
    "    \n",
    "    # Uso de DataLoader con 'num_workers' para paralelizar la carga de datos\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perdida_epoch = 0.0  # Inicializar pérdida de la época\n",
    "        for batch_X, batch_y in dataloader:  # Iterar sobre los batches\n",
    "            optimizer.zero_grad()  # Reiniciar los gradientes\n",
    "            salidas = modelo(batch_X)  # Obtener las salidas del modelo\n",
    "            perdida = criterio(salidas, batch_y)  # Calcular la pérdida\n",
    "            perdida.backward()  # Retropropagación\n",
    "            optimizer.step()  # Actualizar los parámetros\n",
    "            perdida_epoch += perdida.item()  # Acumular pérdida de la época\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Pérdida Promedio: {perdida_epoch/len(dataloader):.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Funciones para inferencia en paralelo\n",
    "# ---------------------------------------------\n",
    "def predecir(modelo, X_data):\n",
    "    \"\"\"\n",
    "    Realiza la inferencia sobre un bloque de datos.\n",
    "    Se utiliza 'torch.no_grad()' para desactivar el cálculo de gradientes.\n",
    "    \"\"\"\n",
    "    modelo.eval()  # Establecer el modelo en modo de evaluación\n",
    "    with torch.no_grad():  # Desactivar el cálculo de gradientes\n",
    "        tensor_X = torch.tensor(X_data, dtype=torch.float32)  # Convertir datos a tensor\n",
    "        salidas = modelo(tensor_X)  # Obtener las salidas del modelo\n",
    "        # Seleccionar la clase con mayor probabilidad\n",
    "        predicciones = torch.argmax(salidas, dim=1).numpy()  # Obtener predicciones\n",
    "    return predicciones\n",
    "\n",
    "def inferencia_paralela(modelo, X_data, num_hilos=4):\n",
    "    \"\"\"\n",
    "    Divide el conjunto de datos en 'num_hilos' partes y realiza la inferencia en paralelo.\n",
    "    Se utiliza ThreadPoolExecutor ya que las operaciones en PyTorch liberan el GIL.\n",
    "    \"\"\"\n",
    "    # Dividir los datos en chunks\n",
    "    total = len(X_data)  # Total de datos\n",
    "    tam_chunk = total // num_hilos  # Tamaño de cada chunk\n",
    "    chunks = [X_data[i*tam_chunk:(i+1)*tam_chunk] for i in range(num_hilos)]  # Crear chunks\n",
    "    \n",
    "    # Si no es divisible exactamente, agregar los datos sobrantes al último chunk\n",
    "    if total % num_hilos != 0:\n",
    "        chunks[-1] = np.concatenate((chunks[-1], X_data[num_hilos*tam_chunk:]), axis=0)  # Agregar sobrantes\n",
    "    \n",
    "    # Uso de ThreadPoolExecutor para paralelizar la inferencia\n",
    "    resultados = [None] * num_hilos  # Inicializar lista de resultados\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_hilos) as executor:\n",
    "        # Se asocia cada chunk con su índice para mantener el orden\n",
    "        futures = {executor.submit(predecir, modelo, chunk): idx for idx, chunk in enumerate(chunks)}\n",
    "        for future in concurrent.futures.as_completed(futures):  # Esperar a que se completen las tareas\n",
    "            idx = futures[future]  # Obtener el índice del chunk\n",
    "            resultados[idx] = future.result()  # Almacenar el resultado\n",
    "    \n",
    "    # Concatenar los resultados y devolver\n",
    "    return np.concatenate(resultados)  # Devolver todas las predicciones\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Función principal\n",
    "# ---------------------------------------------\n",
    "def main():\n",
    "    # Generar datos sintéticos y dividir en entrenamiento y prueba\n",
    "    X, y = generar_datos_sinteticos(num_muestras=5000)  # Generar datos\n",
    "    indice_split = int(0.8 * len(X))  # Índice para dividir los datos\n",
    "    X_train, X_test = X[:indice_split], X[indice_split:]  # Dividir en entrenamiento y prueba\n",
    "    y_train, y_test = y[:indice_split], y[indice_split:]  # Dividir etiquetas\n",
    "    \n",
    "    # Inicializar el modelo\n",
    "    modelo = RedNeuronalSimple(dim_entrada=10, dim_oculta=32, dim_salida=2)  # Crear modelo\n",
    "    \n",
    "    # Entrenamiento del modelo\n",
    "    print(\"Iniciando el entrenamiento del modelo...\")\n",
    "    inicio = time.time()  # Iniciar temporizador\n",
    "    entrenar_modelo(modelo, X_train, y_train, epochs=5, batch_size=128)  # Entrenar modelo\n",
    "    print(f\"Entrenamiento completado en {time.time() - inicio:.2f} segundos.\\n\")  # Mostrar tiempo de entrenamiento\n",
    "    \n",
    "    # Inferencia en paralelo sobre el conjunto de prueba\n",
    "    print(\"Realizando inferencia en paralelo sobre el conjunto de prueba...\")\n",
    "    inicio = time.time()  # Iniciar temporizador\n",
    "    predicciones = inferencia_paralela(modelo, X_test, num_hilos=4)  # Realizar inferencia\n",
    "    print(f\"Inferencia completada en {time.time() - inicio:.2f} segundos.\")  # Mostrar tiempo de inferencia\n",
    "    \n",
    "    # Evaluación de la precisión\n",
    "    precision = np.mean(predicciones == y_test)  # Calcular precisión\n",
    "    print(f\"Precisión en Test: {precision*100:.2f}%\")  # Mostrar precisión\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Ejecutar función principal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
